# Comparison of execution time when running a program on the CPU vs. on the GPU
[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)
[![made-with-latex](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)](https://www.latex-project.org/)

# Abstract
The purpose of this paper is to demonstrate the time difference between running a Python program on the CPU vs. on the GPU when calculating the result of a complex math function. Algorithm implementation is described, along with the math function used in the program. Next, we describe the results of the experiments. In the end, we present a linear regression model for determining the time taken to finish the calculation of arbitrary input size.

# Further reading
Entire document is available [here](document.pdf).